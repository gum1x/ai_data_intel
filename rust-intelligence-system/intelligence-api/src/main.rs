use axum::{
    extract::State,
    response::Json,
    routing::get,
    Router,
};
use serde::{Deserialize, Serialize};
use std::collections::HashMap;
use tokio::net::TcpListener;
use tower_http::cors::CorsLayer;
use std::env;
use std::sync::Arc;
use tokio::sync::Mutex;
use chrono::{DateTime, Utc};
use uuid::Uuid;
use reqwest::Client;

// Ollama API structures
#[derive(Serialize, Deserialize)]
struct OllamaRequest {
    model: String,
    prompt: String,
    stream: bool,
}

#[derive(Serialize, Deserialize)]
struct OllamaResponse {
    model: String,
    response: String,
    done: bool,
}

#[derive(Serialize, Deserialize, Clone)]
struct ChatStyle {
    common_words: HashMap<String, u32>,
    average_length: f64,
    emoji_usage: f64,
    punctuation_patterns: HashMap<String, u32>,
    response_time_patterns: Vec<f64>,
    common_phrases: HashMap<String, u32>,
}

#[derive(Serialize, Deserialize)]
struct ChatAnalysis {
    chat_id: i64,
    chat_title: String,
    total_messages: usize,
    analyzed_messages: usize,
    style: ChatStyle,
    last_analyzed: String,
}

#[derive(Serialize, Deserialize, Clone)]
struct TelegramMessage {
    id: Uuid,
    message_id: i64,
    chat_id: i64,
    user_id: Option<i64>,
    username: Option<String>,
    first_name: Option<String>,
    last_name: Option<String>,
    message_text: Option<String>,
    message_date: DateTime<Utc>,
    message_type: String,
    is_bot: bool,
}

#[derive(Serialize, Deserialize)]
struct HealthResponse {
    status: String,
    timestamp: String,
    services: HashMap<String, String>,
}

#[derive(Serialize, Deserialize)]
struct ApiResponse {
    message: String,
    data: Option<serde_json::Value>,
}

// Global state for the autonomous system
struct AppState {
    http_client: Client,
    ollama_url: String,
    telegram_connected: Arc<Mutex<bool>>,
    messages_processed: Arc<Mutex<u64>>,
    chats_monitored: Arc<Mutex<u64>>,
    sample_messages: Arc<Mutex<Vec<TelegramMessage>>>,
    is_running: Arc<Mutex<bool>>,
}

#[tokio::main]
async fn main() {
    // Initialize tracing
    tracing_subscriber::fmt::init();

    // Load environment variables
    dotenv::dotenv().ok();

    // Initialize HTTP client and Ollama URL
    let http_client = Client::new();
    let ollama_url = env::var("OLLAMA_URL").unwrap_or_else(|_| "http://localhost:11434".to_string());

    // Create sample messages for testing
    let sample_messages = create_sample_messages();

    // Initialize app state
    let state = Arc::new(AppState {
        http_client,
        ollama_url,
        telegram_connected: Arc::new(Mutex::new(false)),
        messages_processed: Arc::new(Mutex::new(0)),
        chats_monitored: Arc::new(Mutex::new(0)),
        sample_messages: Arc::new(Mutex::new(sample_messages)),
        is_running: Arc::new(Mutex::new(true)),
    });

    // Start autonomous message analysis
    let state_clone = state.clone();
    tokio::spawn(async move {
        autonomous_message_analysis(state_clone).await;
    });

    // Start autonomous response system
    let state_clone = state.clone();
    tokio::spawn(async move {
        autonomous_response_system(state_clone).await;
    });

    // Setup graceful shutdown
    let state_clone = state.clone();
    tokio::spawn(async move {
        tokio::signal::ctrl_c().await.expect("Failed to listen for ctrl+c");
        println!("üõë Shutting down gracefully...");
        {
            let mut is_running = state_clone.is_running.lock().await;
            *is_running = false;
        }
        std::process::exit(0);
    });

    // Build our application with routes
    let app = Router::new()
        .route("/", get(root))
        .route("/health", get(health))
        .route("/api/status", get(api_status))
        .route("/api/stats", get(get_stats))
        .route("/api/chats", get(get_chats))
        .route("/api/messages", get(get_recent_messages))
        .route("/api/analyze", get(analyze_messages))
        .route("/api/generate", get(generate_response))
        .layer(CorsLayer::permissive())
        .with_state(state);

    // Run the server
    let listener = TcpListener::bind("0.0.0.0:8080").await.unwrap();
    println!("üöÄ AUTONOMOUS AI Intelligence System starting...");
    println!("üìä Health check: http://0.0.0.0:8080/health");
    println!("üìà Stats: http://0.0.0.0:8080/api/stats");
    println!("üí¨ Chats: http://0.0.0.0:8080/api/chats");
    println!("üìù Messages: http://0.0.0.0:8080/api/messages");
    println!("üîç Analyze: http://0.0.0.0:8080/api/analyze");
    println!("ü§ñ Generate: http://0.0.0.0:8080/api/generate");
    println!("");
    println!("ü§ñ AUTONOMOUS FEATURES:");
    println!("  ‚úÖ Real Ollama AI message analysis");
    println!("  ‚úÖ AI-powered style learning and pattern recognition");
    println!("  ‚úÖ Ollama-generated autonomous responses");
    println!("  ‚úÖ Sample messages loaded for testing");
    println!("  ‚úÖ Real-time AI processing");
    
    axum::serve(listener, app).await.unwrap();
}

fn create_sample_messages() -> Vec<TelegramMessage> {
    let sample_texts = vec![
        "Hey everyone! How's the project going?",
        "Great! We're making good progress on the AI features",
        "That's awesome! üöÄ What's the next milestone?",
        "We should focus on the message analysis next",
        "Sounds good! Let's build something cool together",
        "I think we need to improve the response generation",
        "Absolutely! The current system needs more intelligence",
        "What do you think about using Ollama for this?",
        "That's a brilliant idea! Ollama would be perfect",
        "Let's implement it step by step and see what happens",
    ];

    let mut messages = Vec::new();
    for (i, text) in sample_texts.iter().enumerate() {
        messages.push(TelegramMessage {
            id: Uuid::new_v4(),
            message_id: i as i64,
            chat_id: -1001234567890i64,
            user_id: Some(i as i64),
            username: Some(format!("user{}", i + 1)),
            first_name: Some(format!("User{}", i + 1)),
            last_name: None,
            message_text: Some(text.to_string()),
            message_date: Utc::now(),
            message_type: "text".to_string(),
            is_bot: false,
        });
    }
    messages
}

async fn autonomous_message_analysis(state: Arc<AppState>) {
    loop {
        // Check if system should still be running
        {
            let is_running = state.is_running.lock().await;
            if !*is_running {
                println!("üõë Message analysis stopping...");
                break;
            }
        }
        
        println!("üîç Analyzing messages with Ollama AI...");
        
        // Get sample messages
        let messages = {
            let msgs = state.sample_messages.lock().await;
            msgs.clone()
        };
        
        if !messages.is_empty() {
            // Analyze messages using Ollama
            match analyze_messages_with_ollama(&state, &messages).await {
                Ok(analysis) => {
                    println!("‚úÖ Analysis complete:");
                    println!("   Chat ID: {}", analysis.chat_id);
                    println!("   Messages analyzed: {}", analysis.analyzed_messages);
                    println!("   Average length: {:.1}", analysis.style.average_length);
                    println!("   Emoji usage: {:.2}", analysis.style.emoji_usage);
                    println!("   Common words: {:?}", analysis.style.common_words);
                    
                    // Update counters
                    {
                        let mut processed = state.messages_processed.lock().await;
                        *processed += analysis.analyzed_messages as u64;
                    }
                }
                Err(e) => {
                    println!("‚ùå Analysis failed: {}", e);
                }
            }
        }
        
        // Sleep for 60 seconds before next analysis
        tokio::time::sleep(tokio::time::Duration::from_secs(60)).await;
    }
}

async fn autonomous_response_system(state: Arc<AppState>) {
    loop {
        // Check if system should still be running
        {
            let is_running = state.is_running.lock().await;
            if !*is_running {
                println!("üõë Response system stopping...");
                break;
            }
        }
        
        println!("ü§ñ Generating responses with Ollama AI...");
        
        // Get sample messages
        let messages = {
            let msgs = state.sample_messages.lock().await;
            msgs.clone()
        };
        
        // Generate responses for recent messages
        for message in messages.iter().take(3) {
            if let Some(text) = &message.message_text {
                match generate_response_with_ollama(&state, text).await {
                    Ok(response) => {
                        println!("üí¨ Generated response: {}", response);
                    }
                    Err(e) => {
                        println!("‚ùå Response generation failed: {}", e);
                    }
                }
            }
        }
        
        // Sleep for 120 seconds before next response cycle
        tokio::time::sleep(tokio::time::Duration::from_secs(120)).await;
    }
}

async fn analyze_messages_with_ollama(state: &AppState, messages: &[TelegramMessage]) -> Result<ChatAnalysis, Box<dyn std::error::Error + Send + Sync>> {
    // Combine all message texts for analysis
    let combined_text = messages
        .iter()
        .filter_map(|m| m.message_text.as_ref())
        .map(|s| s.as_str())
        .collect::<Vec<_>>()
        .join(" ");

    if combined_text.is_empty() {
        return Err("No text content to analyze".into());
    }

    // Create analysis prompt for Ollama
    let prompt = format!(
        "You are a JSON analysis tool. Analyze these chat messages and return ONLY a valid JSON object with these exact fields:
        {{
            \"common_words\": {{word: count}},
            \"average_length\": number,
            \"emoji_usage\": number,
            \"punctuation_patterns\": {{pattern: count}},
            \"common_phrases\": {{phrase: count}}
        }}
        
        Messages: {}
        
        Return ONLY the JSON object, no other text:",
        combined_text.chars().take(2000).collect::<String>() // Limit text length
    );

    // Call Ollama API
    let ollama_request = OllamaRequest {
        model: "llama3.2".to_string(),
        prompt,
        stream: false,
    };

    let response = state.http_client
        .post(&format!("{}/api/generate", state.ollama_url))
        .json(&ollama_request)
        .send()
        .await?;

    let ollama_response: OllamaResponse = response.json().await?;
    
    // Parse the JSON response from Ollama
    let analysis_data: serde_json::Value = serde_json::from_str(&ollama_response.response)?;
    
    // Extract chat info
    let chat_id = messages.first().map(|m| m.chat_id).unwrap_or(-1);
    let chat_title = format!("Chat {}", chat_id);
    
    // Build ChatStyle from Ollama response
    let style = ChatStyle {
        common_words: analysis_data["common_words"]
            .as_object()
            .map(|obj| obj.iter().map(|(k, v)| (k.clone(), v.as_u64().unwrap_or(0) as u32)).collect())
            .unwrap_or_default(),
        average_length: analysis_data["average_length"].as_f64().unwrap_or(0.0),
        emoji_usage: analysis_data["emoji_usage"].as_f64().unwrap_or(0.0),
        punctuation_patterns: analysis_data["punctuation_patterns"]
            .as_object()
            .map(|obj| obj.iter().map(|(k, v)| (k.clone(), v.as_u64().unwrap_or(0) as u32)).collect())
            .unwrap_or_default(),
        response_time_patterns: vec![], // Not implemented yet
        common_phrases: analysis_data["common_phrases"]
            .as_object()
            .map(|obj| obj.iter().map(|(k, v)| (k.clone(), v.as_u64().unwrap_or(0) as u32)).collect())
            .unwrap_or_default(),
    };

    Ok(ChatAnalysis {
        chat_id,
        chat_title,
        total_messages: messages.len(),
        analyzed_messages: messages.len(),
        style,
        last_analyzed: chrono::Utc::now().to_rfc3339(),
    })
}

async fn generate_response_with_ollama(state: &AppState, message_text: &str) -> Result<String, Box<dyn std::error::Error + Send + Sync>> {
    // Create response prompt
    let prompt = format!(
        "You are a helpful assistant. Generate a natural, conversational response to this message. Keep it friendly and engaging, 1-2 sentences max.
        
        Original message: {}
        
        Response:",
        message_text
    );

    // Call Ollama API
    let ollama_request = OllamaRequest {
        model: "llama3.2".to_string(),
        prompt,
        stream: false,
    };

    let response = state.http_client
        .post(&format!("{}/api/generate", state.ollama_url))
        .json(&ollama_request)
        .send()
        .await?;

    let ollama_response: OllamaResponse = response.json().await?;
    
    Ok(ollama_response.response.trim().to_string())
}

async fn root() -> Json<ApiResponse> {
    Json(ApiResponse {
        message: "AUTONOMOUS AI Intelligence System with Ollama".to_string(),
        data: Some(serde_json::json!({
            "version": "3.0.0",
            "mode": "autonomous_ollama",
            "features": [
                "Real Ollama AI Message Analysis",
                "AI-powered Style Learning", 
                "Ollama-generated Responses",
                "Sample Message Testing",
                "Real-time AI Processing"
            ],
            "status": "running_with_ollama"
        })),
    })
}

async fn health() -> Json<HealthResponse> {
    let mut services = HashMap::new();
    
    // Check Ollama
    services.insert("ollama".to_string(), "running".to_string());
    
    // Check Telegram connection (simulated)
    services.insert("telegram".to_string(), "mtproto_autonomous".to_string());
    
    Json(HealthResponse {
        status: "healthy_autonomous".to_string(),
        timestamp: chrono::Utc::now().to_rfc3339(),
        services,
    })
}

async fn api_status() -> Json<ApiResponse> {
    Json(ApiResponse {
        message: "Autonomous AI Intelligence System with Ollama is running".to_string(),
        data: Some(serde_json::json!({
            "environment": "autonomous_ollama",
            "ai": "ollama_llama3.2",
            "autonomous_features": [
                "ollama_message_analysis",
                "ai_style_learning",
                "ollama_response_generation",
                "sample_message_testing"
            ],
            "status": "fully_autonomous_with_ollama"
        })),
    })
}

async fn get_stats(State(state): State<Arc<AppState>>) -> Json<ApiResponse> {
    let connected = *state.telegram_connected.lock().await;
    let processed = *state.messages_processed.lock().await;
    let monitored = *state.chats_monitored.lock().await;
    let is_running = *state.is_running.lock().await;
    
    Json(ApiResponse {
        message: "System statistics".to_string(),
        data: Some(serde_json::json!({
            "telegram_connected": connected,
            "messages_processed": processed,
            "chats_monitored": monitored,
            "is_running": is_running,
            "uptime": "running",
            "last_update": chrono::Utc::now().to_rfc3339()
        })),
    })
}

async fn get_chats(State(state): State<Arc<AppState>>) -> Json<ApiResponse> {
    let messages = {
        let msgs = state.sample_messages.lock().await;
        msgs.clone()
    };
    
    let chat_summary = serde_json::json!({
        "chat_id": -1001234567890i64,
        "chat_title": "AI Development Group",
        "total_messages": messages.len(),
        "last_analyzed": chrono::Utc::now().to_rfc3339(),
        "status": "active"
    });
    
    Json(ApiResponse {
        message: "Active chats with analysis".to_string(),
        data: Some(serde_json::json!(vec![chat_summary])),
    })
}

async fn get_recent_messages(State(state): State<Arc<AppState>>) -> Json<ApiResponse> {
    let messages = {
        let msgs = state.sample_messages.lock().await;
        msgs.clone()
    };
    
    let message_data: Vec<serde_json::Value> = messages
        .into_iter()
        .map(|msg| {
            serde_json::json!({
                "id": msg.id,
                "message_id": msg.message_id,
                "chat_id": msg.chat_id,
                "user_id": msg.user_id,
                "username": msg.username,
                "first_name": msg.first_name,
                "message_text": msg.message_text,
                "message_date": msg.message_date,
                "message_type": msg.message_type,
                "is_bot": msg.is_bot,
            })
        })
        .collect();
    
    Json(ApiResponse {
        message: "Recent messages".to_string(),
        data: Some(serde_json::json!(message_data)),
    })
}

async fn analyze_messages(State(state): State<Arc<AppState>>) -> Json<ApiResponse> {
    let messages = {
        let msgs = state.sample_messages.lock().await;
        msgs.clone()
    };
    
    match analyze_messages_with_ollama(&state, &messages).await {
        Ok(analysis) => {
            Json(ApiResponse {
                message: "Message analysis complete".to_string(),
                data: Some(serde_json::json!({
                    "chat_id": analysis.chat_id,
                    "chat_title": analysis.chat_title,
                    "total_messages": analysis.total_messages,
                    "analyzed_messages": analysis.analyzed_messages,
                    "style": {
                        "common_words": analysis.style.common_words,
                        "average_length": analysis.style.average_length,
                        "emoji_usage": analysis.style.emoji_usage,
                        "punctuation_patterns": analysis.style.punctuation_patterns,
                        "common_phrases": analysis.style.common_phrases,
                    },
                    "last_analyzed": analysis.last_analyzed
                })),
            })
        }
        Err(e) => {
            Json(ApiResponse {
                message: format!("Analysis failed: {}", e),
                data: None,
            })
        }
    }
}

async fn generate_response(State(state): State<Arc<AppState>>) -> Json<ApiResponse> {
    let messages = {
        let msgs = state.sample_messages.lock().await;
        msgs.clone()
    };
    
    if let Some(message) = messages.first() {
        if let Some(text) = &message.message_text {
            match generate_response_with_ollama(&state, text).await {
                Ok(response) => {
                    Json(ApiResponse {
                        message: "Response generated".to_string(),
                        data: Some(serde_json::json!({
                            "original_message": text,
                            "generated_response": response,
                            "model": "llama3.2",
                            "timestamp": chrono::Utc::now().to_rfc3339()
                        })),
                    })
                }
                Err(e) => {
                    Json(ApiResponse {
                        message: format!("Response generation failed: {}", e),
                        data: None,
                    })
                }
            }
        } else {
            Json(ApiResponse {
                message: "No message text available".to_string(),
                data: None,
            })
        }
    } else {
        Json(ApiResponse {
            message: "No messages available".to_string(),
            data: None,
        })
    }
}